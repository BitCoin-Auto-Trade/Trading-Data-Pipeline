version: "3.8"

x-airflow-env: &airflow-env
  AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: ${AIRFLOW__DATABASE__SQL_ALCHEMY_CONN}
  AIRFLOW__CORE__EXECUTOR: ${AIRFLOW__CORE__EXECUTOR}
  AIRFLOW__CORE__FERNET_KEY: ${FERNET_KEY}

  # 코드 경로
  PYTHONPATH: ${PYTHONPATH}

  # S3
  AWS_ACCESS_KEY_ID: ${AWS_ACCESS_KEY_ID}
  AWS_SECRET_ACCESS_KEY: ${AWS_SECRET_ACCESS_KEY}
  AWS_S3_BUCKET: ${AWS_S3_BUCKET}
  AWS_REGION: ${AWS_REGION}

  # Snowflake
  SNOWFLAKE_ACCOUNT: ${SNOWFLAKE_ACCOUNT}
  SNOWFLAKE_USER: ${SNOWFLAKE_USER}
  SNOWFLAKE_PASSWORD: ${SNOWFLAKE_PASSWORD}
  SNOWFLAKE_DATABASE: ${SNOWFLAKE_DATABASE}
  SNOWFLAKE_SCHEMA: ${SNOWFLAKE_SCHEMA}
  SNOWFLAKE_WAREHOUSE: ${SNOWFLAKE_WAREHOUSE}
  SNOWFLAKE_STAGE: ${SNOWFLAKE_STAGE}
  SNOWFLAKE_FILE_FORMAT: ${SNOWFLAKE_FILE_FORMAT}

services:
  airflow-init:
    image: apache/airflow:3.0.1-python3.10
    environment:
      <<: *airflow-env
    volumes:
      - ./dags:/opt/airflow/dags
      - ./logs:/opt/airflow/logs
      - ./src:/opt/airflow/src
    entrypoint: >
      bash -c "
        airflow db migrate &&
        airflow users create --username admin --password admin --firstname admin --lastname admin --role Admin --email admin@example.com
      "
    restart: "no"
    mem_limit: 512m
    cpus: 0.3

  airflow-webserver:
    image: apache/airflow:3.0.1-python3.10
    restart: always
    depends_on:
      - airflow-init
    ports:
      - "8080:8080"
    environment:
      <<: *airflow-env
    volumes:
      - ./dags:/opt/airflow/dags
      - ./logs:/opt/airflow/logs
      - ./src:/opt/airflow/src
    command: api-server
    mem_limit: 1500m
    cpus: 0.5

  airflow-scheduler:
    image: apache/airflow:3.0.1-python3.10
    restart: always
    depends_on:
      - airflow-webserver
    environment:
      <<: *airflow-env
    volumes:
      - ./dags:/opt/airflow/dags
      - ./logs:/opt/airflow/logs
      - ./src:/opt/airflow/src
    command: scheduler
    mem_limit: 600m
    cpus: 0.4
